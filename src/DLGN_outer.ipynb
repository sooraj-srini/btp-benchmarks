{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kjNxUX51Lh0k"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "%reset -f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from itertools import product as cartesian_prod\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "def sigmoid(u):\n",
    "    u = np.maximum(u,-100)\n",
    "    u = np.minimum(u,100)\n",
    "    return 1/(1+np.exp(-u))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import pairwise_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.numlayer=4\n",
    "        self.numnodes=12\n",
    "        self.beta=5.\n",
    "        self.lr=1.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4_12_50_1.0e-03\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "args =  Args()\n",
    "\n",
    "num_layer = args.numlayer\n",
    "num_neuron = args.numnodes\n",
    "beta = args.beta\n",
    "lr=args.lr\n",
    "\n",
    "saved_epochs = list(range(0,300,1)) + list(range(300,10001,50))\n",
    "update_value_epochs = list(range(0,10001,100))# \n",
    "filename_suffix = str(num_layer)\n",
    "filename_suffix += \"_\"+str(num_neuron)\n",
    "filename_suffix += \"_\"+str(int(beta))\n",
    "filename_suffix += \"_\"+format(lr,\".1e\")\n",
    "print(filename_suffix)\n",
    "\n",
    "\n",
    "no_of_batches=10 #[1,10,100]\n",
    "weight_decay=0.0\n",
    "num_hidden_nodes=[num_neuron]*num_layer\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDx4xoSOFzR2"
   },
   "source": [
    "**Variable parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "2iXNxcu4L6kT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/w8bnb5rs76g9lp6n7cpjt8ch0000gn/T/ipykernel_52862/2315234387.py:85: DeprecationWarning: This function is deprecated. Please call randint(0, 10000 + 1) instead\n",
      "  seeds = np.random.random_integers(0,10000,100)\n"
     ]
    }
   ],
   "source": [
    "#@title Synthetic data\n",
    "def set_npseed(seed):\n",
    "\tnp.random.seed(seed)\n",
    "\n",
    "\n",
    "def set_torchseed(seed):\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed_all(seed)\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\ttorch.backends.cudnn.benchmark = False\n",
    "\n",
    "#Four mode classification data\n",
    "\n",
    "\n",
    "def data_gen_decision_tree(num_data=1000, dim=2, seed=0, w_list=None, b_list=None, \n",
    "\t\t\t\t\t\t\tvals=None, num_levels=2):\n",
    "\t# np.random.seed(6790)\n",
    "\tset_npseed(seed=seed)\n",
    "\t# Construct a complete decision tree with 2**num_levels-1 internal nodes, \n",
    "\t# e.g. num_levels=2 means there are 3 internal nodes.\n",
    "\t# w_list, b_list is a list of size equal to num_internal_nodes\n",
    "\t# vals is a list of size equal to num_leaf_nodes, with values +1 or -1\n",
    "\tnum_internal_nodes = 2**num_levels - 1\n",
    "\tnum_leaf_nodes = 2**num_levels\n",
    "\tstats = np.zeros(num_internal_nodes+num_leaf_nodes)\n",
    "\n",
    "\tif vals is None:\n",
    "\t\tvals = np.arange(0,num_internal_nodes+num_leaf_nodes,1,dtype=np.int32)%2\n",
    "\t\tvals[:num_internal_nodes] = -99\n",
    "\n",
    "\tif w_list is None:\n",
    "\t\tw_list = np.random.standard_normal((num_internal_nodes, dim))\n",
    "\t\tw_list = w_list/np.linalg.norm(w_list, axis=1)[:, None]\n",
    "\t\tb_list = np.zeros((num_internal_nodes))\n",
    "\tw_list = np.zeros((num_internal_nodes, dim))\n",
    "\tfor i in range(num_internal_nodes):\n",
    "\t\tw_list[i,i]=1.\n",
    "\n",
    "\tdata_x = np.random.standard_normal((num_data, dim))\n",
    "\tdata_x /= np.sqrt(np.sum(data_x**2, axis=1, keepdims=True))\n",
    "\trelevant_stats = data_x @ w_list.T + b_list\n",
    "\t\n",
    "\n",
    "\tcurr_index = np.zeros(shape=(num_data), dtype=int)\n",
    "\t\n",
    "\tfor level in range(num_levels):\n",
    "\t\tnodes_curr_level=list(range(2**level - 1,2**(level+1)-1  ))\n",
    "\t\tfor el in nodes_curr_level:\n",
    "\t\t\trelevant_stats[:,el] += b_list[el]\n",
    "\t\tdecision_variable = np.choose(curr_index, relevant_stats.T) \n",
    "\t\t# Go down and right if wx+b>0 down and left otherwise. \n",
    "\t\t# i.e. 0 -> 1 if w[0]x+b[0]<0 and 0->2 otherwise\n",
    "\t\tcurr_index = (curr_index+1)*2 - (1-(decision_variable > 0))\n",
    "\n",
    "\tbound_dist = np.min(np.abs(relevant_stats), axis=1)\n",
    "\tthres = 0.005\n",
    "\t# thres =0 \n",
    "\tlabels = vals[curr_index]\n",
    "\tdata_x_pruned = data_x[bound_dist>thres]\n",
    "\tlabels_pruned = labels[bound_dist>thres]\n",
    "\trelevant_stats = np.sign(data_x_pruned @ w_list.T + b_list)\n",
    "\tnodes_active = np.zeros((len(data_x_pruned),  num_internal_nodes+num_leaf_nodes), dtype=np.int32)\n",
    "\tfor node in range(num_internal_nodes+num_leaf_nodes):\n",
    "\t\tif node==0:\n",
    "\t\t\tstats[node]=len(relevant_stats)\n",
    "\t\t\tnodes_active[:,0]=1\n",
    "\t\t\tcontinue\n",
    "\t\tparent = (node-1)//2\n",
    "\t\tnodes_active[:,node]=nodes_active[:,parent]\n",
    "\t\tright_child = node-(parent*2)-1 # 0 means left, 1 means right 1 has children 3,4\n",
    "\t\tif right_child==1:\n",
    "\t\t\tnodes_active[:,node] *= relevant_stats[:,parent]>0\n",
    "\t\tif right_child==0:\n",
    "\t\t\tnodes_active[:,node] *= relevant_stats[:,parent]<0\t\t\n",
    "\t\tstats = nodes_active.sum(axis=0)\n",
    "\treturn ((data_x_pruned, labels_pruned), (w_list, b_list, vals), stats)\n",
    "\n",
    "\n",
    "# w_list = np.array([[1., 0], [0, 1], [0, 1]])\n",
    "# b_list = np.array([0, 0.25, -0.25])\n",
    "# vals = np.array([-99, -99, -99, 0, 1, 1, 0])\n",
    "num_data = 100000\n",
    "input_dim=100\n",
    "seeds = np.random.random_integers(0,10000,100)\n",
    "seeds=[1387]\n",
    "# seeds = [2318]\n",
    "for seed in seeds:\n",
    "\t((data_x, labels), (w_list, b_list, vals), stats) = data_gen_decision_tree(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tdim=input_dim, seed=seed, num_levels=4,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tnum_data=num_data)\n",
    "\tseed_set=seed\n",
    "w_list_old = np.array(w_list)\n",
    "b_list_old = np.array(b_list)\n",
    "\n",
    "num_data = len(data_x)\n",
    "num_train= num_data//2\n",
    "num_vali = num_data//4\n",
    "num_test = num_data//4\n",
    "train_data = data_x[:num_train,:]\n",
    "train_data_labels = labels[:num_train]\n",
    "\n",
    "vali_data = data_x[num_train:num_train+num_vali,:]\n",
    "vali_data_labels = labels[num_train:num_train+num_vali]\n",
    "\n",
    "test_data = data_x[num_train+num_vali :,:]\n",
    "test_data_labels = labels[num_train+num_vali :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27236, 100)\n",
      "(27236,)\n",
      "(13618, 100)\n",
      "(13618,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(train_data_labels.shape)\n",
    "\n",
    "print(test_data.shape)\n",
    "print(test_data_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DLGN_FC(nn.Module):\n",
    "\tdef __init__(self, input_dim=None, output_dim=None, num_hidden_nodes=[], beta=30, mode='pwc'):\t\t\n",
    "\t\tsuper(DLGN_FC, self).__init__()\n",
    "\t\tself.num_hidden_layers = len(num_hidden_nodes)\n",
    "\t\tself.beta=beta  # Soft gating parameter\n",
    "\t\tself.mode = mode\n",
    "\t\tself.num_nodes=[input_dim]+num_hidden_nodes+[output_dim]\n",
    "\t\tself.gating_layers=nn.ModuleList()\n",
    "\t\tself.value_layers=nn.Parameter(torch.randn([1]+num_hidden_nodes)/100.)\n",
    "\t\tself.num_layer = len(num_hidden_nodes)\n",
    "\t\tself.num_hidden_nodes = num_hidden_nodes\n",
    "\t\tfor i in range(self.num_hidden_layers+1):\n",
    "\t\t\tif i!=self.num_hidden_layers:\n",
    "\t\t\t\ttemp = nn.Linear(self.num_nodes[0], self.num_nodes[i+1], bias=False)\n",
    "\t\t\t\tself.gating_layers.append(temp)\n",
    "\n",
    "\tdef set_parameters_with_mask(self, to_copy, parameter_masks):\n",
    "\t\t# self and to_copy are DLGN_FC objects with same architecture\n",
    "\t\t# parameter_masks is compatible with dict(to_copy.named_parameters())\n",
    "\t\tfor (name, copy_param) in to_copy.named_parameters():\n",
    "\t\t\tcopy_param = copy_param.clone().detach()\n",
    "\t\t\torig_param  = self.state_dict()[name]\n",
    "\t\t\tif name in parameter_masks:\n",
    "\t\t\t\tparam_mask = parameter_masks[name]>0\n",
    "\t\t\t\torig_param[param_mask] = copy_param[param_mask]\n",
    "\t\t\telse:\n",
    "\t\t\t\torig_param = copy_param.data.detach()\n",
    "\t\n",
    "\n",
    "\t\t\t\t\t\t\t\t\n",
    "\n",
    "\tdef return_gating_functions(self):\n",
    "\t\teffective_weights = []\n",
    "\t\tfor i in range(self.num_hidden_layers):\n",
    "\t\t\tcurr_weight = self.gating_layers[i].weight.detach().clone()\n",
    "\t\t\t# curr_weight /= torch.norm(curr_weight, dim=1, keepdim=True)\n",
    "\t\t\teffective_weights.append(curr_weight)\n",
    "\t\treturn effective_weights\n",
    "\t\t# effective_weights (and effective biases) is a list of size num_hidden_layers\n",
    "\t\t\t\t\t\t\t\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t\n",
    "\n",
    "\t\tfor el in self.parameters():\n",
    "\t\t\tif el.is_cuda:\n",
    "\t\t\t\tdevice = torch.device('cuda:1')\n",
    "\t\t\telse:\n",
    "\t\t\t\tdevice = torch.device('cpu')\n",
    "\t\tvalues=[torch.ones(x.shape).to(device)]\n",
    "\t\t\n",
    "\t\t\n",
    "\t\tfor i in range(self.num_hidden_layers):\n",
    "\t\t\tfiber = [len(x)]+[1]*self.num_layer\n",
    "\t\t\tfiber[i+1] = self.num_hidden_nodes[i]\n",
    "\t\t\tfiber = tuple(fiber)\n",
    "\t\t\tgate_score = torch.sigmoid( self.beta*(x@self.gating_layers[i].weight.T))#/\n",
    "\t\t\t    #   torch.norm(self.gating_layers[i].weight, dim=1, keepdim=True).T) \n",
    "\t\t\tgate_score = gate_score.reshape(fiber) \n",
    "\t\t\tif i==0:\n",
    "\t\t\t\tcp = gate_score\n",
    "\t\t\telse:\n",
    "\t\t\t\tcp = cp*gate_score \n",
    "\t\treturn torch.sum(cp*self.value_layers, dim=(1,2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ncr5k6koMbD_"
   },
   "outputs": [],
   "source": [
    "#@title Train DLGN model\n",
    "def train_dlgn (DLGN_obj, train_data_curr,vali_data_curr,test_data_curr,\n",
    "\t\t\t\ttrain_labels_curr,test_labels_curr,vali_labels_curr,\n",
    "\t\t\t\tparameter_mask=dict()):\n",
    "\t# DLGN_obj is the initial network\n",
    "\t# parameter_mask is a dictionary compatible with dict(DLGN_obj.named_parameters())\n",
    "\t# if a key corresponding to a named_parameter is not present it is assumed to be all ones (i.e it will be updated)\n",
    "\t\n",
    "\t# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\t\n",
    "\t# Speed up of a factor of over 40 by using GPU instead of CPU\n",
    "\t# Final train loss of 0.02 and test acc of 74%\n",
    "\tdevice = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\t# device = torch.device('cpu')\n",
    "\tDLGN_obj.to(device)\n",
    "\n",
    "\tcriterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\toptimizer = optim.SGD(DLGN_obj.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "\ttrain_data_torch = torch.Tensor(train_data_curr)\n",
    "\tvali_data_torch = torch.Tensor(vali_data_curr)\n",
    "\ttest_data_torch = torch.Tensor(test_data_curr)\n",
    "\n",
    "\ttrain_labels_torch = torch.tensor(train_labels_curr, dtype=torch.int64)\n",
    "\ttest_labels_torch = torch.tensor(test_labels_curr, dtype=torch.int64)\n",
    "\tvali_labels_torch = torch.tensor(vali_labels_curr, dtype=torch.int64)\n",
    "\n",
    "\tnum_batches = no_of_batches\n",
    "\tbatch_size = len(train_data_curr)//num_batches\n",
    "\tlosses=[]\n",
    "\tDLGN_obj_store = []\n",
    "\tbest_vali_error = len(vali_labels_curr)\n",
    "\t\n",
    "\n",
    "\t# print(\"H3\")\n",
    "\t# print(DLGN_params)\n",
    "\tdebug_models= []\n",
    "\ttrain_losses = []\n",
    "\ttepoch = tqdm(range(saved_epochs[-1]+1))\n",
    "\tfor epoch in tepoch:  # loop over the dataset multiple times\n",
    "\t\tif epoch in update_value_epochs:\n",
    "\t\t\t# updating the value pathdim vector by optimising \n",
    "\n",
    "\t\t\ttrain_preds =DLGN_obj(torch.Tensor(train_data_curr).to(device)).reshape((-1,1))\n",
    "\t\t\tcriterion = nn.CrossEntropyLoss()\n",
    "\t\t\toutputs = torch.cat((-1*train_preds,train_preds), dim=1)\n",
    "\t\t\ttargets = torch.tensor(train_labels_curr, dtype=torch.int64).to(device)\n",
    "\t\t\t\n",
    "\t\t\ttrain_loss = criterion(outputs, targets)\n",
    "\t\t\tprint(\"Loss lefore updating value_net at epoch\", epoch, \" is \", train_loss)\n",
    "\t\t\tprint(\"Total path abs value\", torch.abs(DLGN_obj.value_layers.cpu().detach()).sum().numpy())\n",
    "\n",
    "\t\t\tew = DLGN_obj.return_gating_functions()\n",
    "\t\t\tcp_feat1 = sigmoid(beta*np.dot(train_data_curr,ew[0].cpu().T).reshape(-1,num_neuron,1,1,1))\n",
    "\t\t\tcp_feat2 = sigmoid(beta*np.dot(train_data_curr,ew[1].cpu().T).reshape(-1,1,num_neuron,1,1))\n",
    "\t\t\tcp_feat3 = sigmoid(beta*np.dot(train_data_curr,ew[2].cpu().T).reshape(-1,1,1,num_neuron,1))\n",
    "\t\t\tcp_feat4 = sigmoid(beta*np.dot(train_data_curr,ew[3].cpu().T).reshape(-1,1,1,1,num_neuron))\n",
    "\t\t\tcp_feat = cp_feat1 * cp_feat2 * cp_feat3 * cp_feat4\n",
    "\t\t\tcp_feat_vec = cp_feat.reshape((len(cp_feat),-1))\n",
    "\n",
    "\t\t\tclf = LogisticRegression(C=0.03, fit_intercept=False,max_iter=1000, penalty=\"l1\", solver='liblinear')\n",
    "\t\t\tclf.fit(2*cp_feat_vec, train_labels_curr)\n",
    "\t\t\tvalue_wts  = clf.decision_function(np.eye(num_neuron**num_layer)).reshape(1,num_neuron,num_neuron,num_neuron, num_neuron)\n",
    "\t\t\t\n",
    "\t\t\tA= DLGN_obj.value_layers.detach()\n",
    "\t\t\tA[:] = torch.Tensor(value_wts)\n",
    "\n",
    "\t\t\ttrain_preds =DLGN_obj(torch.Tensor(train_data_curr).to(device)).reshape((-1,1))\n",
    "\t\t\tcriterion = nn.CrossEntropyLoss()\n",
    "\t\t\toutputs = torch.cat((-1*train_preds,train_preds), dim=1)\n",
    "\t\t\ttargets = torch.tensor(train_labels_curr, dtype=torch.int64).to(device)\n",
    "\t\t\ttrain_loss = criterion(outputs, targets)\n",
    "\t\t\tprint(\"Loss after updating value_net at epoch\", epoch, \" is \", train_loss)\t\t\t\n",
    "\t\t\tprint(\"Total path abs value\", torch.abs(DLGN_obj.value_layers.cpu().detach()).sum().numpy())\n",
    "\t\tif epoch in saved_epochs:\n",
    "\t\t\tDLGN_obj_copy = deepcopy(DLGN_obj)\n",
    "\t\t\tDLGN_obj_copy.to(torch.device('cpu'))\n",
    "\t\t\tDLGN_obj_store.append(DLGN_obj_copy)\n",
    "\t\t\n",
    "\t\tfor batch_start in range(0,len(train_data_curr),batch_size):\n",
    "\t\t\tif (batch_start+batch_size)>len(train_data_curr):\n",
    "\t\t\t\tbreak\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tinputs = train_data_torch[batch_start:batch_start+batch_size]\n",
    "\t\t\ttargets = train_labels_torch[batch_start:batch_start+batch_size].reshape(batch_size)\n",
    "\t\t\tcriterion = nn.CrossEntropyLoss()\n",
    "\t\t\tinputs = inputs.to(device)\n",
    "\t\t\ttargets = targets.to(device)\n",
    "\t\t\tpreds = DLGN_obj(inputs).reshape(-1,1)\n",
    "\t\t\t# preds_clone = preds.detach().clone().cpu().numpy()[:,0]\n",
    "\t\t\t# targets_clone = targets.detach().clone().cpu().numpy()\n",
    "\t\t\t# coeff = (0.5-targets_clone)/(sigmoid(2*preds_clone)-targets_clone)\n",
    "\t\t\t# print(coeff.shape)\n",
    "\t\t\t\n",
    "\t\t\t# print(coeff.min())\n",
    "\t\t\t# print(coeff.mean())\n",
    "\t\t\t# print(coeff.max())\n",
    "\t\t\toutputs = torch.cat((-1*preds, preds), dim=1)\n",
    "\t\t\tloss = criterion(outputs, targets)\n",
    "\t\t\t# loss = loss*torch.tensor(coeff, device=device)\t\n",
    "\t\t\t# loss = loss.mean()\t\t\n",
    "\t\t\tloss.backward()\n",
    "\t\t\tfor name,param in DLGN_obj.named_parameters():\n",
    "\t\t\t\tif \"val\" in name:\n",
    "\t\t\t\t\tparam.grad *= 0.0\n",
    "\t\t\t\tif \"gat\" in name:\n",
    "\t\t\t\t\tparam.grad *= 1.0\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\ttrain_preds =DLGN_obj(torch.Tensor(train_data_curr).to(device)).reshape(-1,1)\n",
    "\t\tcriterion = nn.CrossEntropyLoss()\n",
    "\t\toutputs = torch.cat((-1*train_preds,train_preds), dim=1)\n",
    "\t\ttargets = torch.tensor(train_labels_curr, dtype=torch.int64).to(device)\n",
    "\t\ttrain_loss = criterion(outputs, targets)\n",
    "\t\tif epoch%5 == 0:\n",
    "\t\t\tprint(\"Loss after updating at epoch \", epoch, \" is \", train_loss)\n",
    "\t\t\ttest_preds =DLGN_obj(test_data_torch.to(device)).reshape(-1,1)\n",
    "\t\t\ttest_preds = test_preds.detach().cpu().numpy()\n",
    "\t\t\tprint(\"Test error=\",np.sum(test_labels_curr != (np.sign(test_preds[:,0])+1)//2 ))\n",
    "\t\tif train_loss < 0.005:\n",
    "\t\t\tbreak\n",
    "\t\tif np.isnan(train_loss.detach().cpu().numpy()):\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\tlosses.append(train_loss.cpu().detach().clone().numpy())\n",
    "\t\tinputs = vali_data_torch.to(device)\n",
    "\t\ttargets = vali_labels_torch.to(device)\n",
    "\t\tpreds =DLGN_obj(inputs).reshape(-1,1)\n",
    "\t\tvali_preds = torch.cat((-1*preds, preds), dim=1)\n",
    "\t\tvali_preds = torch.argmax(vali_preds, dim=1)\n",
    "\t\tvali_error= torch.sum(targets!=vali_preds)\n",
    "\t\tif vali_error < best_vali_error:\n",
    "\t\t\tDLGN_obj_return = deepcopy(DLGN_obj)\n",
    "\t\t\tbest_vali_error = vali_error\n",
    "\tplt.figure()\n",
    "\tplt.title(\"DLGN loss vs epoch\")\n",
    "\tplt.plot(losses)\n",
    "\tif not os.path.exists('figures'):\n",
    "\t\tos.mkdir('figures')\n",
    "\n",
    "\tfilename = 'figures/'+filename_suffix +'.pdf'\n",
    "\tplt.savefig(filename)\n",
    "\tDLGN_obj_return.to(torch.device('cpu'))\n",
    "\tdevice = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\t# device = torch.device('cpu')\n",
    "\treturn train_losses, DLGN_obj_return, DLGN_obj_store, losses, debug_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpN8Yby7Fllw"
   },
   "source": [
    "**Training a DLGN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "LF7PO4Lc2jzV",
    "outputId": "5858b9a2-48bb-4fdb-9d55-f37981c6f839"
   },
   "outputs": [],
   "source": [
    "set_torchseed(41972)\n",
    "# set_torchseed(5612)\n",
    "DLGN_init= DLGN_FC(input_dim=input_dim, output_dim=1, num_hidden_nodes=num_hidden_nodes, beta=beta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,parameter in DLGN_init.named_parameters():\n",
    "    print(name)\n",
    "    print(parameter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parameter_masks=dict()\n",
    "for name,parameter in DLGN_init.named_parameters():\n",
    "    if \"val\" in name:\n",
    "        train_parameter_masks[name]=torch.ones_like(parameter)# Updating all value network layers\n",
    "    if \"gat\" in name:\n",
    "        train_parameter_masks[name]=torch.ones_like(parameter)\n",
    "    train_parameter_masks[name].to(device)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "set_torchseed(5000)\n",
    "train_losses, DLGN_obj_final, DLGN_obj_store, losses , debug_models= train_dlgn(train_data_curr=train_data,\n",
    "                                            vali_data_curr=vali_data,\n",
    "                                            test_data_curr=test_data,\n",
    "                                            train_labels_curr=train_data_labels,\n",
    "                                            vali_labels_curr=vali_data_labels,\n",
    "                                            test_labels_curr=test_data_labels,\n",
    "                                            DLGN_obj=deepcopy(DLGN_init),\n",
    "                                            parameter_mask=train_parameter_masks,\n",
    "                                            )\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "losses=np.array(losses)\n",
    "\n",
    "print(DLGN_obj_store[-1].beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('outputs'):\n",
    "    os.mkdir('outputs')\n",
    "device=torch.device('cpu')\n",
    "train_preds =DLGN_obj_final(torch.Tensor(train_data).to(device)).reshape(-1,1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "outputs = torch.cat((-1*train_preds,train_preds), dim=1)\n",
    "targets = torch.tensor(train_data_labels, dtype=torch.int64)\n",
    "train_loss = criterion(outputs, targets)\n",
    "train_preds = train_preds.detach().numpy()\n",
    "filename = 'outputs/'+filename_suffix+'.txt'\n",
    "original_stdout = sys.stdout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename,'w') as f:\n",
    "    sys.stdout = f\n",
    "    print(\"Setup:\")\n",
    "    print(\"Num neurons : \", DLGN_obj_final.num_nodes)\n",
    "    print(\" Beta :\", DLGN_obj_final.beta)\n",
    "    print(\" lr :\", lr)\n",
    "    print(\"=======================\")\n",
    "    print(train_losses)\n",
    "    print(\"==========Best validated model=============\")\n",
    "    print(\"Train error=\",np.sum(train_data_labels != (np.sign(train_preds[:,0])+1)//2 ))\n",
    "    print(\"Train loss = \", train_loss)\n",
    "    print(\"Num_train_data=\",len(train_data_labels))\n",
    "    sys.stdout = original_stdout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds =DLGN_obj_final(torch.Tensor(test_data)).reshape(-1,1)\n",
    "test_preds = test_preds.detach().numpy()\n",
    "filename = 'outputs/'+filename_suffix+'.txt'\n",
    "original_stdout = sys.stdout\n",
    "with open(filename,'a') as f:\n",
    "    sys.stdout = f\n",
    "    print(\"Test error=\",np.sum(test_data_labels != (np.sign(test_preds[:,0])+1)//2 ))\n",
    "    print(\"Num_test_data=\",len(test_data_labels))\n",
    "    sys.stdout = original_stdout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list = np.concatenate((w_list_old,-w_list_old),axis=0)\n",
    "\n",
    "effective_weights = DLGN_obj_store[0].return_gating_functions()\n",
    "wts_list_init=[]\n",
    "for layer in range(0,len(effective_weights)):\n",
    "    wts =  np.array(effective_weights[layer].detach().clone().numpy())+0.001\n",
    "    wts /= np.linalg.norm(wts, axis=1)[:,None]\n",
    "    wts_list_init.append(wts)\n",
    "wts_list_init = np.concatenate(wts_list_init)\n",
    "\n",
    "\n",
    "effective_weights = DLGN_obj_final.return_gating_functions()\n",
    "if np.isnan(train_loss.detach().numpy()):\n",
    "    filename = 'outputs/'+filename_suffix+'.txt'\n",
    "    original_stdout = sys.stdout\n",
    "    with open(filename,'a') as f:\n",
    "        sys.stdout = f\n",
    "        print(\"Nan error\")\n",
    "        sys.stdout = original_stdout\n",
    "else:\n",
    "    wts_list=[]\n",
    "    for layer in range(len(effective_weights)):\n",
    "        wts =  np.array(effective_weights[layer].detach().clone().numpy()) + 0.001\n",
    "        wts /= np.linalg.norm(wts, axis=1)[:,None]\n",
    "        wts_list.append(wts)\n",
    "    wts_list = np.concatenate(wts_list)\n",
    "\n",
    "    pd0 =  pairwise_distances(w_list,wts_list_init)\n",
    "    pd1 =  pairwise_distances(w_list,wts_list)\n",
    "\n",
    "\n",
    "    filename = 'outputs/'+filename_suffix+'.txt'\n",
    "    original_stdout = sys.stdout\n",
    "    with open(filename,'a') as f:\n",
    "        sys.stdout = f\n",
    "        print(\"Shape of decision tree node hyperplanes \", w_list.shape)\n",
    "        print(\"Shape of all halfspace directions of DLGN\", wts_list.shape)\n",
    "        print(\"Distance of closest init DLGN halfspace to each labelling func hyperplane \\n\", pd0.min(axis=1)[:len(w_list_old)])\n",
    "        print(pd0.min(axis=1)[len(w_list_old):])\n",
    "        print(\"Distance of closest lrnd DLGN halfspace to each labelling func hyperplane \\n\", pd1.min(axis=1)[:len(w_list_old)])\n",
    "        print(pd1.min(axis=1)[len(w_list_old):])\n",
    "        print(\"Number of halfspaces within distance 0.8 of the Dtree hyperplanes \\n\", np.sum(pd1<0.8, axis=1)[:len(w_list_old)])\n",
    "        print(np.sum(pd1<0.8, axis=1)[len(w_list_old):])\n",
    "        print(\"Number of halfspaces within distance 0.6 of the Dtree hyperplanes \\n\", np.sum(pd1<0.6, axis=1)[:len(w_list_old)])\n",
    "        print(np.sum(pd1<0.6, axis=1)[len(w_list_old):])\n",
    "        print(\"Number of halfspaces within distance 0.4 of the Dtree hyperplanes \\n\", np.sum(pd1<0.4, axis=1)[:len(w_list_old)])\n",
    "        print(np.sum(pd1<0.4, axis=1)[len(w_list_old):])\n",
    "        print(\"Number of halfspaces within distance 0.3 of the Dtree hyperplanes \\n\", np.sum(pd1<0.3, axis=1)[:len(w_list_old)])\n",
    "        print(np.sum(pd1<0.3, axis=1)[len(w_list_old):])\n",
    "        print(\"Number of halfspaces within distance 0.2 of the Dtree hyperplanes \\n\", np.sum(pd1<0.2, axis=1)[:len(w_list_old)])\n",
    "        print(np.sum(pd1<0.2, axis=1)[len(w_list_old):])\n",
    "        print(\"Number of halfspaces within distance 0.1 of the Dtree hyperplanes \\n\", np.sum(pd1<0.1, axis=1)[:len(w_list_old)])\n",
    "        print(np.sum(pd1<0.1, axis=1)[len(w_list_old):])\n",
    "        print(\"=========================================\")\n",
    "        sys.stdout = original_stdout\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_index in tqdm(range(len(DLGN_obj_store))):\n",
    "    train_preds =DLGN_obj_store[epoch_index](torch.Tensor(train_data).to(device)).reshape(-1,1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    outputs = torch.cat((-1*train_preds,train_preds), dim=1)\n",
    "    targets = torch.tensor(train_data_labels, dtype=torch.int64)\n",
    "    train_loss = criterion(outputs, targets)\n",
    "\n",
    "\n",
    "    if np.isnan(train_loss.detach().numpy()):\n",
    "        filename = 'outputs/'+filename_suffix+'.txt'\n",
    "        original_stdout = sys.stdout\n",
    "        with open(filename,'a') as f:\n",
    "            sys.stdout = f\n",
    "            print(\"Nan error\")\n",
    "            sys.stdout = original_stdout\n",
    "            continue\n",
    "\n",
    "    effective_weights = DLGN_obj_store[epoch_index].return_gating_functions()\n",
    "    wts_list=[]\n",
    "    wts_list_unnorm=[]\n",
    "    for layer in range(len(effective_weights)):\n",
    "        wts =  np.array(effective_weights[layer].detach().clone().numpy())+0.001\n",
    "        wts_list_unnorm.append(np.array(wts))\n",
    "        wts /= np.linalg.norm(wts, axis=1)[:,None]\n",
    "        wts_list.append(wts)\n",
    "    wts_list = np.concatenate(wts_list)\n",
    "    wts_list_unnorm = np.concatenate(wts_list_unnorm)\n",
    "    pd0 =  pairwise_distances(w_list,wts_list_init)\n",
    "    pd1 =  pairwise_distances(w_list,wts_list)\n",
    "    pd_w_list = pairwise_distances(w_list, w_list)\n",
    "    w_list_random = np.random.standard_normal(w_list.shape)\n",
    "    w_list_random /= np.linalg.norm(w_list_random, axis=1)[:,None]\n",
    "\n",
    "    pd4 = pairwise_distances(w_list_random,wts_list)\n",
    "\n",
    "\n",
    "    filename = 'outputs/'+filename_suffix+'.txt'\n",
    "    original_stdout = sys.stdout\n",
    "    with open(filename,'a') as f:\n",
    "        sys.stdout = f\n",
    "        print(\"===================================\")\n",
    "        print(epoch_index)\n",
    "        print(saved_epochs[epoch_index])\n",
    "        print(\"===================================\")\n",
    "        train_preds = train_preds.detach().numpy()\n",
    "        test_preds =DLGN_obj_store[epoch_index](torch.Tensor(test_data)).reshape(-1,1)\n",
    "        test_preds = test_preds.detach().numpy()\n",
    "        print(\"Train error=\",np.sum(train_data_labels != (np.sign(train_preds[:,0])+1)//2 ))\n",
    "        print(\"Num_train_data=\",len(train_data_labels))\n",
    "        print(\"Train loss=\",train_loss.detach())\n",
    "        print(\"Test error=\",np.sum(test_data_labels != (np.sign(test_preds[:,0])+1)//2 ))\n",
    "        print(\"Num_test_data=\",len(test_data_labels))\n",
    "        print(\"===================================\")\n",
    "\n",
    "        print(\"Shape of decision tree node hyperplanes \", w_list.shape)\n",
    "        print(\"Shape of all halfspace directions of DLGN\", wts_list.shape)\n",
    "        print(\"Distance of closest init DLGN halfspace to each labelling func hyperplane \\n\", pd0.min(axis=1)[:len(w_list_old)])\n",
    "        print(pd0.min(axis=1)[len(w_list_old):])\n",
    "        print(\"Distance of closest lrnd DLGN halfspace to each labelling func hyperplane \\n\", pd1.min(axis=1)[:len(w_list_old)])\n",
    "        print(pd1.min(axis=1)[len(w_list_old):])\n",
    "        print(\"Number of halfspaces within distance 0.8 of the Dtree hyperplanes \\n\", np.sum(pd1<0.8, axis=1)[:len(w_list_old)])\n",
    "        print(np.sum(pd1<0.8, axis=1)[len(w_list_old):])\n",
    "        print(\"Number of halfspaces within distance 0.6 of the Dtree hyperplanes \\n\", np.sum(pd1<0.6, axis=1)[:len(w_list_old)])\n",
    "        print(np.sum(pd1<0.6, axis=1)[len(w_list_old):])\n",
    "        print(\"Number of halfspaces within distance 0.4 of the Dtree hyperplanes \\n\", np.sum(pd1<0.4, axis=1)[:len(w_list_old)])\n",
    "        print(np.sum(pd1<0.4, axis=1)[len(w_list_old):])\n",
    "        print(\"Number of halfspaces within distance 0.3 of the Dtree hyperplanes \\n\", np.sum(pd1<0.3, axis=1)[:len(w_list_old)])\n",
    "        print(np.sum(pd1<0.3, axis=1)[len(w_list_old):])\n",
    "        print(\"Number of halfspaces within distance 0.2 of the Dtree hyperplanes \\n\", np.sum(pd1<0.2, axis=1)[:len(w_list_old)])\n",
    "        print(np.sum(pd1<0.2, axis=1)[len(w_list_old):])\n",
    "        print(\"Number of halfspaces within distance 0.1 of the Dtree hyperplanes \\n\", np.sum(pd1<0.1, axis=1)[:len(w_list_old)])\n",
    "        print(np.sum(pd1<0.1, axis=1)[len(w_list_old):])\n",
    "        print(\"=========================================\")\n",
    "        print(\"============Norm, max, argmax of hidden neurons, index, sign at argmax===========\")\n",
    "        a = np.linalg.norm(wts_list_unnorm, axis=1)\n",
    "        b = np.max(np.abs(wts_list_unnorm), axis=1)\n",
    "        c = np.argmax(np.abs(wts_list_unnorm), axis=1) \n",
    "        d = np.argsort(-a)\n",
    "        for i in range(len(a)):\n",
    "            print(a[d[i]], b[d[i]], c[d[i]], d[i],np.sign(wts_list_unnorm[d[i],c[d[i]]]) )\n",
    "        print(\"=========================================\")\n",
    "        sys.stdout = original_stdout    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ew_store = np.zeros((len(DLGN_obj_store),num_layer,num_neuron,input_dim))\n",
    "val_store = np.zeros( [len(DLGN_obj_store)]+ [num_neuron]*num_layer )\n",
    "for i,model in enumerate(DLGN_obj_store):\n",
    "    ew = DLGN_obj_store[i].return_gating_functions()\n",
    "    ew_store[i,0] = ew[0].numpy()\n",
    "    ew_store[i,1] = ew[1].numpy()\n",
    "    ew_store[i,2] = ew[2].numpy()\n",
    "    ew_store[i,3] = ew[3].numpy()\n",
    "    val_store[i] = model.value_layers.detach().clone().numpy()\n",
    "# np.savez(\"npzfiles/DLGN_outer_100dim_microanalysis_init_zero_thres.npz\", ew_store, val_store)\n",
    "\n",
    "# print(ew_store.shape)\n",
    "# print(val_store.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chumma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = np.load(\"npzfiles/DLGN_outer_100dim_microanalysis_init.npz\")\n",
    "# arrays = np.load(\"npzfiles/DLGN_outer_100dim_microanalysis_init_zero_thres.npz\")\n",
    "ew_store_outer = arrays[\"arr_0\"]\n",
    "val_store_outer = arrays[\"arr_1\"]\n",
    "print(ew_store_outer.shape)\n",
    "print(val_store_outer.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.sum(np.abs(val_store_outer), axis=0)\n",
    "print(np.sum(temp>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_curve = []\n",
    "test_loss_curve = []\n",
    "test_error_curve = []\n",
    "train_loss_vecs = []\n",
    "for epoch_index in range(len(saved_epochs)):\n",
    "    DLGN_obj = DLGN_FC(input_dim=input_dim, output_dim=1, num_hidden_nodes=num_hidden_nodes, beta=beta)\n",
    "    temp = deepcopy(val_store_outer[epoch_index])\n",
    "    # temp[np.abs(temp)<0.1]=0\n",
    "    DLGN_obj.value_layers.data = torch.Tensor(temp)\n",
    "    DLGN_obj.gating_layers[0].weight.data = torch.Tensor(deepcopy(ew_store_outer[epoch_index,0]))\n",
    "    DLGN_obj.gating_layers[1].weight.data = torch.Tensor(deepcopy(ew_store_outer[epoch_index,1]))\n",
    "    DLGN_obj.gating_layers[2].weight.data = torch.Tensor(deepcopy(ew_store_outer[epoch_index,2]))\n",
    "    DLGN_obj.gating_layers[3].weight.data = torch.Tensor(deepcopy(ew_store_outer[epoch_index,3]))\n",
    "    device = torch.device(\"cpu\")\n",
    "    DLGN_obj.to(device)\n",
    "\n",
    "    train_preds = DLGN_obj(torch.Tensor(train_data).to(device)).detach().clone().numpy()\n",
    "    train_loss_vec = -np.log(sigmoid(train_preds*2*(2*train_data_labels - 1)))\n",
    "    test_preds = DLGN_obj(torch.Tensor(test_data).to(device)).detach().clone().numpy()\n",
    "    test_loss_vec = -np.log(sigmoid(test_preds*2*(2*test_data_labels - 1)))\n",
    "    train_loss_vecs.append(train_loss_vec)\n",
    "    train_loss_curve.append(train_loss_vec.mean())\n",
    "    test_loss_curve.append(test_loss_vec.mean())\n",
    "    test_error_curve.append(np.sum(test_loss_vec>np.log(2))) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(saved_epochs, train_loss_curve)\n",
    "plt.plot(saved_epochs, test_loss_curve)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(saved_epochs,test_error_curve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.linalg.norm(ew_store_outer, axis=3)\n",
    "b= np.max(np.abs(ew_store_outer), axis=3)\n",
    "c= np.argmax(np.abs(ew_store_outer), axis=3)\n",
    "epoch_index = -1\n",
    "ew_store_sign = np.sign(ew_store_outer)\n",
    "large_val_paths = np.where(np.abs(val_store_outer[epoch_index])>0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in [1]:\n",
    "    for k in range(num_neuron):\n",
    "        plt.figure()\n",
    "        plt.title(\"l=\"+str(l)+\" k=\"+str(k)+\" argmax = \"+str(c[-1,l,k]))\n",
    "        for i in range(15):\n",
    "            plt.plot(saved_epochs, ew_store_outer[:,l,k,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select013715   = (train_data[:,0]<0) * (train_data[:,1]<0) * (train_data[:,3]<0) *  (train_data[:,7]<0) #\n",
    "select013716   = (train_data[:,0]<0) * (train_data[:,1]<0) * (train_data[:,3]<0) *  (train_data[:,7]>0) #\n",
    "select013817   = (train_data[:,0]<0) * (train_data[:,1]<0) * (train_data[:,3]>0) *  (train_data[:,8]<0) #\n",
    "select013818   = (train_data[:,0]<0) * (train_data[:,1]<0) * (train_data[:,3]>0) *  (train_data[:,8]>0) #\n",
    "select014919   = (train_data[:,0]<0) * (train_data[:,1]>0) * (train_data[:,4]<0) *  (train_data[:,9]<0) #\n",
    "select0149120  = (train_data[:,0]<0) * (train_data[:,1]>0) * (train_data[:,4]<0) *  (train_data[:,9]>0) #\n",
    "select01491021 = (train_data[:,0]<0) * (train_data[:,1]>0) * (train_data[:,4]>0) * (train_data[:,10]<0) #\n",
    "select01491022 = (train_data[:,0]<0) * (train_data[:,1]>0) * (train_data[:,4]>0) * (train_data[:,10]>0) #\n",
    "\n",
    "select0251123   = (train_data[:,0]>0) * (train_data[:,2]<0) * (train_data[:,5]<0) * (train_data[:,11]<0) #\n",
    "select0251124   = (train_data[:,0]>0) * (train_data[:,2]<0) * (train_data[:,5]<0) * (train_data[:,11]>0) #\n",
    "select0251225   = (train_data[:,0]>0) * (train_data[:,2]<0) * (train_data[:,5]>0) * (train_data[:,12]<0) #\n",
    "select0251226   = (train_data[:,0]>0) * (train_data[:,2]<0) * (train_data[:,5]>0) * (train_data[:,12]>0) #\n",
    "select0261327   = (train_data[:,0]>0) * (train_data[:,2]>0) * (train_data[:,6]<0) * (train_data[:,13]<0) #\n",
    "select0261328   = (train_data[:,0]>0) * (train_data[:,2]>0) * (train_data[:,6]<0) * (train_data[:,13]>0) #\n",
    "select0261429   = (train_data[:,0]>0) * (train_data[:,2]>0) * (train_data[:,6]>0) * (train_data[:,14]<0) #\n",
    "select0261430   = (train_data[:,0]>0) * (train_data[:,2]>0) * (train_data[:,6]>0) * (train_data[:,14]>0) #\n",
    "\n",
    "select_subsets_train_leaves = [select013715, select013716, select013817, select013818, select014919, select0149120, select01491021, select01491022,\n",
    "                               select0251123, select0251124,  select0251225, select0251226, select0261327, select0261328, select0261429, select0261430 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l,n = 0,0\n",
    "\n",
    "plt.plot(saved_epochs[:310], np.abs(ew_store_outer[:310,l,n,0]), \"-\");\n",
    "plt.plot(saved_epochs[:310], np.abs(ew_store_outer[:310,l,n,1:15]), \"-\");\n",
    "plt.plot(saved_epochs[:310], np.abs(ew_store_outer[:310,l,n,15:100]), \"--\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_index=-1\n",
    "mult=1.\n",
    "ew =  ew_store_outer[epoch_index]\n",
    "cp_feat1 = sigmoid(mult*beta*np.dot(train_data,ew[0].T).reshape(-1,num_neuron,1,1,1))\n",
    "cp_feat2 = sigmoid(mult*beta*np.dot(train_data,ew[1].T).reshape(-1,1,num_neuron,1,1))\n",
    "cp_feat3 = sigmoid(mult*beta*np.dot(train_data,ew[2].T).reshape(-1,1,1,num_neuron,1))\n",
    "cp_feat4 = sigmoid(mult*beta*np.dot(train_data,ew[3].T).reshape(-1,1,1,1,num_neuron))\n",
    "cp_feat = cp_feat1 * cp_feat2 * cp_feat3 * cp_feat4\n",
    "cp_feat_vec = cp_feat.reshape((len(cp_feat),-1))\n",
    "cp_feat_vec_select = cp_feat_vec[:,np.abs(val_store_outer[epoch_index].reshape(-1))>0.00000001]\n",
    "large_val_paths = np.where(np.abs(val_store_outer[epoch_index])>0.00000001)\n",
    "\n",
    "large_vals = np.zeros(len(large_val_paths[0]))\n",
    "for i in range(len(large_val_paths[0])):\n",
    "    i1 = large_val_paths[0][i]\n",
    "    i2 = large_val_paths[1][i]\n",
    "    i3 = large_val_paths[2][i]\n",
    "    i4 = large_val_paths[3][i]\n",
    "    large_vals[i] = val_store_outer[epoch_index,i1,i2,i3,i4]\n",
    "    print(\"============================================\")\n",
    "    print(c[-1,0,i1], c[-1,1,i2], c[-1,2,i3], c[-1,3,i4])\n",
    "    print(b[-1,0,i1], b[-1,1,i2], b[-1,2,i3], b[-1,3,i4])\n",
    "    print(ew_store_outer[-1,0,i1,c[-1,0,i1]], ew_store_outer[-1,1,i2,c[-1,1,i2]],\n",
    "           ew_store_outer[-1,2,i3,c[-1,2,i3]], ew_store_outer[-1,3,i4,c[-1,3,i4]])\n",
    "    print(a[-1,0,i1], a[-1,1,i2], a[-1,2,i3], a[-1,3,i4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_statistics= list()\n",
    "for l in range(4):\n",
    "    for i1 in [0]:\n",
    "        for i2 in range(12):\n",
    "            for i3 in range(12):\n",
    "                for i4 in range(12):\n",
    "                    if np.abs(val_store_outer[epoch_index,i1,i2,i3,i4])<0.000000001:\n",
    "                        continue\n",
    "                \n",
    "                    select1 = cp_feat1[:,i1,0,0,0]>0.5\n",
    "                    select2 = cp_feat2[:,0,i2,0,0]>0.5\n",
    "                    select3 = cp_feat3[:,0,0,i3,0]>0.5\n",
    "                    select4 = cp_feat4[:,0,0,0,i4]>0.5\n",
    "\n",
    "                    if l==0:\n",
    "                        selects = [select2, select3, select4, (cp_feat1[:,i1,0,0,0]>0.1)&(cp_feat1[:,i1,0,0,0]<0.9), select1 ]                    \n",
    "                    if l==1:\n",
    "                        selects = [select1, select3, select4, (cp_feat2[:,0,i2,0,0]>0.1)&(cp_feat2[:,0,i2,0,0]<0.9), select2]\n",
    "                    if l==2:\n",
    "                        selects = [select1, select2, select4, (cp_feat3[:,0,0,i3,0]>0.1)&(cp_feat3[:,0,0,i3,0]<0.9), select3]\n",
    "                    if l==3:\n",
    "                        selects = [select1, select2, select3, (cp_feat4[:,0,0,0,i4]>0.1)&(cp_feat4[:,0,0,0,i4]<0.9), select4]\n",
    "\n",
    "                    entry =dict()\n",
    "                    entry['l'] = l\n",
    "                    entry['i1'] = i1\n",
    "                    entry['i2'] = i2\n",
    "                    entry['i3'] = i3\n",
    "                    entry['i4'] = i4\n",
    "                    entry['val'] = val_store_outer[epoch_index,i1,i2,i3,i4]\n",
    "                    entry['num_data_thru'] = np.sum(selects[0]*selects[1]*selects[2]*selects[3] )\n",
    "                    entry[\"argmax layer 1\"] = c[epoch_index,0,i1]\n",
    "                    entry[\"argmax layer 2\"] = c[epoch_index,1,i2]\n",
    "                    entry[\"argmax layer 3\"] = c[epoch_index,2,i3]\n",
    "                    entry[\"argmax layer 4\"] = c[epoch_index,3,i4]\n",
    "                    entry[\"max layer1\"] = ew_store_outer[epoch_index,0,i1,c[epoch_index,0,i1]]\n",
    "                    entry[\"max layer2\"] = ew_store_outer[epoch_index,1,i2,c[epoch_index,1,i2]]\n",
    "                    entry[\"max layer3\"] = ew_store_outer[epoch_index,2,i3,c[epoch_index,2,i3]]\n",
    "                    entry[\"max layer4\"] = ew_store_outer[epoch_index,3,i4,c[epoch_index,3,i4]]\n",
    "\n",
    "                    selected_data = train_data[selects[0]*selects[1]*selects[2]*selects[3]]\n",
    "                    selected_data_labels = train_data_labels[selects[0]*selects[1]*selects[2]*selects[3]]\n",
    "                    selected_data_cp_feat_vec_select = cp_feat_vec_select[selects[0]*selects[1]*selects[2]*selects[3]]\n",
    "                    selected_data_preds = sigmoid(2*np.dot(selected_data_cp_feat_vec_select,large_vals))\n",
    "                    selected_data_grad_imp = (selected_data_preds - selected_data_labels)/(0.5 - selected_data_labels)\n",
    "\n",
    "                    preds = sigmoid(2*np.dot(cp_feat_vec_select,large_vals))\n",
    "                    grad_imp = (preds - train_data_labels)/(0.5 - train_data_labels)\n",
    "\n",
    "                    entry[\"Conf_mats_ODT_node_classifiers\"] = np.zeros((16,2,2))\n",
    "\n",
    "                    for i in range(16):\n",
    "                        active_sensitive_indices_right = selects[0]*selects[1]*selects[2]*selects[3]* (train_data[:,i]>0)\n",
    "                        active_sensitive_indices_left = selects[0]*selects[1]*selects[2]*selects[3]* (train_data[:,i]<0)\n",
    "                        num_pos_right = np.sum(train_data_labels[active_sensitive_indices_right]* grad_imp[active_sensitive_indices_right])\n",
    "                        num_neg_right = np.sum( (1-train_data_labels[active_sensitive_indices_right]) * grad_imp[active_sensitive_indices_right] )\n",
    "                        num_pos_left = np.sum(train_data_labels[active_sensitive_indices_left] * grad_imp[active_sensitive_indices_left])\n",
    "                        num_neg_left = np.sum( (1-train_data_labels[active_sensitive_indices_left]) *  grad_imp[active_sensitive_indices_left])\n",
    "                        entry[\"Conf_mats_ODT_node_classifiers\"][i,0,0] = num_neg_left\n",
    "                        entry[\"Conf_mats_ODT_node_classifiers\"][i,0,1] = num_neg_right\n",
    "                        entry[\"Conf_mats_ODT_node_classifiers\"][i,1,0] = num_pos_left\n",
    "                        entry[\"Conf_mats_ODT_node_classifiers\"][i,1,1] = num_pos_right\n",
    "\n",
    "                    entry[\"Sum improvement\"] = np.zeros((2,16))\n",
    "                    for i in range(16):\n",
    "                        entry[\"Sum improvement\"][0,i] = entry[\"val\"]* (entry[\"Conf_mats_ODT_node_classifiers\"][i,1,0] - entry[\"Conf_mats_ODT_node_classifiers\"][i,0,0])\n",
    "                        entry[\"Sum improvement\"][1,i] = entry[\"val\"]* (entry[\"Conf_mats_ODT_node_classifiers\"][i,1,1] - entry[\"Conf_mats_ODT_node_classifiers\"][i,0,1])\n",
    "\n",
    "\n",
    "                    path_statistics.append(entry)\n",
    "                    \n",
    "selected_path_stats = [el for el in path_statistics if el[\"l\"]==0 and el[\"i1\"]==0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=0\n",
    "neuron_index = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(precision=3)\n",
    "sum_improvement_total = np.zeros((2,16))\n",
    "print(len(selected_path_stats))\n",
    "for j in range(len(selected_path_stats)):\n",
    "    path_stat = selected_path_stats[j]\n",
    "    print(\"Neuron index\", path_stat[\"i1\"], path_stat[\"i2\"], path_stat[\"i3\"], path_stat[\"i4\"])\n",
    "    print(\"Argmax of gating HS\", path_stat[\"argmax layer 1\"], path_stat[\"argmax layer 2\"], path_stat[\"argmax layer 3\"], path_stat[\"argmax layer 4\"])\n",
    "    print(\"Max of gating HS\",path_stat[\"max layer1\"], path_stat[\"max layer2\"], path_stat[\"max layer3\"], path_stat[\"max layer4\"])\n",
    "    print(\"Path value\",  path_stat[\"val\"])\n",
    "    print(\"confmat 0\", \"\\n\", path_stat[\"Conf_mats_ODT_node_classifiers\"][0])\n",
    "    print(\"confmat 1\", \"\\n\", path_stat[\"Conf_mats_ODT_node_classifiers\"][1])\n",
    "    print(\"confmat 15\", \"\\n\", path_stat[\"Conf_mats_ODT_node_classifiers\"][15])\n",
    "    print(\" Sum Improvement\", \"\\n\", path_stat[\"Sum improvement\"] )\n",
    "    print(\"==========\")\n",
    "    sum_improvement_total += path_stat[\"Sum improvement\"]\n",
    "grads = sum_improvement_total[1,:]-sum_improvement_total[0,:]\n",
    "print(\"Total improvement: \\n\", grads )\n",
    "curr = ew_store_outer[epoch_index,layer, neuron_index, :16]\n",
    "print(\"Current HS: \\n\", curr)\n",
    "print(\"Normalised tot improv.: \\n\", grads/np.linalg.norm(grads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_index=99\n",
    "mult=1.\n",
    "ew =  ew_store_outer[epoch_index]\n",
    "cp_feat1 = sigmoid(mult*beta*np.dot(train_data,ew[0].T).reshape(-1,num_neuron,1,1,1))\n",
    "cp_feat2 = sigmoid(mult*beta*np.dot(train_data,ew[1].T).reshape(-1,1,num_neuron,1,1))\n",
    "cp_feat3 = sigmoid(mult*beta*np.dot(train_data,ew[2].T).reshape(-1,1,1,num_neuron,1))\n",
    "cp_feat4 = sigmoid(mult*beta*np.dot(train_data,ew[3].T).reshape(-1,1,1,1,num_neuron))\n",
    "cp_feat = cp_feat1 * cp_feat2 * cp_feat3 * cp_feat4\n",
    "cp_feat_vec = cp_feat.reshape((len(cp_feat),-1))\n",
    "cp_feat_vec_select = cp_feat_vec[:,np.abs(val_store_outer[epoch_index].reshape(-1))>0.00000001]\n",
    "large_val_paths = np.where(np.abs(val_store_outer[epoch_index])>0.00000001)\n",
    "large_vals = val_store_outer[epoch_index][large_val_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.00\n",
    "cp_feat_vec_select_thresholded = np.zeros(cp_feat_vec_select.shape)\n",
    "cp_feat_vec_select_thresholded[cp_feat_vec_select>tau]  = cp_feat_vec_select[cp_feat_vec_select>tau]\n",
    "train_preds = np.dot(cp_feat_vec_select_thresholded, large_vals)\n",
    "plt.title(\"Mult =\"+str(mult)+\" Epoch index = \"+str(epoch_index))\n",
    "plt.hist(train_preds[train_data_labels==0], bins=50);\n",
    "plt.hist(train_preds[train_data_labels==1], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( ((sigmoid(2*train_preds[train_data_labels==1]) - 1.)/(0.5-1)) .mean())\n",
    "print( ((sigmoid(2*train_preds[(train_data_labels==1) & (np.abs(train_data[:,0])<0.03)]) - 1)/(0.5-1)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.hist(train_preds[train_data_labels==1], bins=50);\n",
    "plt.figure()\n",
    "plt.hist(train_preds[(train_data_labels==1) & (np.abs(train_data[:,0]<0.04))], bins=50);"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "00ad5f1807eee938f7727b558c9158a01118eae9a3a444b82c1137c2e4c2794d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
